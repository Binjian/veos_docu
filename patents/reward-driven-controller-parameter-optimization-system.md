---
title: 发明和实用新型技术交底书-软硬结合案 
author: 忻斌健
mainfont: Noto Sans Mono CJK SC
CJKoptions: BoldFont=STHeiti, ItalicFont=STKaiti, PunctStyle=kaiming, XeTexlinebreaklocale "zh" 
---

# 基于大数据和能效奖励驱动的车辆控制器参数优化方法 
## 背景技术

### 技术领域,背景以及相关技术发展

传统控制器设计方法是根据专家系统，按预定的规则，设计控制器的参数，调节控制器的输出，以及保证控制器的稳定性。这些方法通常基于经典的反馈控制理论和最优控制理论，针对特定的控制指标和目标函数,设计出一系列的控制器参数，保证控制器的输出和稳定性。除了控制器的输出和稳定性的要求，不同的应用场景通常还会有其他复杂和高阶的性能要求，比如安全性，舒适性等，特别是节能和能效指标。这就要求设计独立于现有控制器设计方案的控制器参数优化方法，在不影响现有控制器设计方案的前提下，设计并且评估不同的控制器参数，兼顾原有控制指标（控制器输出和稳定性）以及其他复杂高阶的性能指标。

由于强化学习方法训练和收集经验的特点，利用强化学习方法解决应用的世界问题目前主要是在游戏和仿真系统领域。用于实际系统控制上应用非常有限，目前较为知名的应用问题还有深度神经网络架构搜索，数据中心最优空调制冷控制方案，集成电路最优布线设计。

### 发明最接近的现有技术

现有车辆控制系统对节能和能效指标的要求主要是也是通过专家系统实现。这种专家系统的通常做法需要定义并识别场景，基于离散的规则按车辆运动状态如车速等划分离散的场景，针对每个场景按相应的规则设计控制器参数，并且设计控制器输出，以及保证控制器的稳定性。

第二种方法是模仿学习。这种方法是通过研究和分析不同司机的驾驶数据，通过关联分析能耗数据，得出驾驶数据和能效指标之间的关联，通过监督学习的方式获得能效最佳的油门和刹车操作。

第三种方法是利用强化学习方法根据车辆的运动状态调节混合动力车的电动力和内燃机动力的最优分配比，通过奖励驱动优化达到自适应保持电机和内燃机在最优工作区域达到节能目的。

## 现有技术的问题

1. 第一种方法的缺点是需要定义并识别离散场景类。因为实际场景中的车速等状态量是连续的，这样强行划分场景就不可避免造成人为误差，在给识别离散场景类的算法带来了一定难度的同时，还造成了优化策略实际上只是近似。在实际应用中，离散场景类的定义和识别是需要较长时间的，这样的话，算法的设计就会变得比较复杂和随意，无法达到最优的目标。更重要的是，这种方法实际上是一种随即开环控制，无法利用环境产生的动态观测量及时更新控制策略。另外，专家系统对测量数据的使用效率不高。

2. 第二种模仿学习的方法需要研究司机的驾驶行为，进行数据收集分类和识别，另外模仿学习是替代司机对车辆油门，刹车踏板的控制，会干预司机的正常驾驶，造成不可控的安全问题。

3.	第三种方法目前的应用场景还主要是混动车领域，尚未在纯电动车或者内燃机车上应用。而且它的提高能效的主要方法并不是直接调节车端的控制器，而是通过混动车的动力分配比，仅适用于混动车一个特殊控制方法。

## 技术方案的发明概述

本技术方案根据观测量动态调整参数适应不同工况 ，按文献【1】在决策方法的定义以及对系统观测状态的编码方法可按两种类型的控制策略实施：

1. 一型系统（System 1）：这种类型的决策系统依据的短期的观测量（几秒内）选择当前的控制策略，不考虑长期的策略。在强化学习的实现中可避免复杂的长效策略优化和功劳归属问题，减少推理和训练系统的复杂度和计算量。
2. 二型系统（System 2）：这种类型的决策系统通过对长时间段观测量的编码，实现对较长期效应的系统状态的识别。


系统由三个模块构成：

1. 车端动力域控制器: 
   - 执行实时的动态最优决策; 
   - 向云端上传车辆的运动状态数据，
   司机对车辆的控制接口（刹车，油门，方向盘）的操作状态；
   - 车辆的能耗数据。

2. 云端计算集群：
   - 训练
   - 推理
   - 决策推送

3. 数据传输系统：
   - 车载信息系统（TBox）
   - 信息系统服务供应商（TSP）  

本技术方案按应用方式具体有三种应用场景：

1. 无智能驾驶系统的普通车辆基于能效奖励驱动的车辆控制器（动力控制器）参数优化系统。系统由两个环路组成，一个是常规车辆动力控制环路，另一个是动力控制器参数优化环路。

2. 智能驾驶系统基于能效奖励驱动的车辆控制器（动力控制器）参数优化系统。系统由两个环路组成，一个智能驾驶（包括辅助驾驶和自动驾驶）纵向控制环路，另一个是动力控制器参数优化环路。

3. 智能驾驶系统基于能效奖励驱动的智能驾驶纵向控制器参数优化系统。系统由两个环路组成，一个智能驾驶（包括辅助驾驶和自动驾驶）控制环路，另一个是动力控制器参数优化环路。



## 技术方案的详细阐述

1. 系统方案

- 车端动力域控制器: 
   - 执行实时的动态最优决策; 
   - 向云端上传车辆的运动状态数据，
   司机对车辆的控制接口（刹车，油门，方向盘）的操作状态；
   - 车辆的能耗数据。

- 云端计算集群：
   - 训练
   - 推理
   - 决策推送

- 数据传输系统：
   - 车载信息系统（TBox）
   - 信息系统服务供应商（TSP）  
系统方案如图1所示。

图1 系统方案



2. 数据要求及流水， 数据需要满足以下要求：

- 可观测性: 可反映能耗的影响因素
   - 车辆运动状态：车速
   - 油门刹车踏板状态（驾驶员对车辆的运动规划）
   - 不需要导航数据和其他运动规划数据
- 可控性: 可有效影响能耗的可优化空间
   - 电动力系统控制器的参数

系统数据流水如图2所示。

图2 数据流水

3. 机器学习方法
采用奖励驱动的强化学习方法，比如但不限于策略梯度方法（Policy Gradient)或者基于Q学习的方法（Q-Learning)，或者是基于模型的方法（Model-Based Reinforcement Learning)。

4. 应用场景

   4.1 无智能驾驶系统的普通车辆基于能效奖励驱动的车辆控制器（动力控制器）参数优化系统,如图3所示。系统由两个环路组成, 一个是常规车辆控制环路,另一个是控制器参数优化环路。

      - 常规车辆控制环路由司机正常驾驶车辆操作油门踏板和刹车踏板;动力域控制器控制器接收油门踏板和刹车踏板信号,通过计算获得需要的请求扭矩,输出到动力系统, 动力系统执行动力域控制器的指令驱动车辆

      - 控制器参数优化环路主要由智能体(也称为智能决策模块)及其输入输出接口构成.智能体根据车辆运动状态,油门踏板和刹车踏板状态,在满足常规控制环路控制指标的前提下,按照最优的降低能耗的目标动态对动力域控制器相关参数(比如刹车踏板请求扭矩映射表)进行调整.调整的方法可以是当前最先进或者典型的奖励驱动的强化学习方法比如基于策略梯度的方法(Policy Graident Method)或者基于Q学习的方法(Q-Learning Method)或者是基于模型的强化学习方法.
图 3

   4.2 智能驾驶系统基于能效奖励驱动的车辆控制器（动力控制器）参数优化系统。如图4所示。系统由两个环路组成, 一个是智能驾驶(包括辅助或者自动车辆)控制环路,另一个是控制器参数优化环路.
   
      - 智能驾驶控制环路由智能驾驶纵向控制器按运动规划算法计算出期望速度或加速信号或减速信号请求值发送给动力域控制器;动力域控制器控制器请求的速度或加速度或减速度信号,通过计算获得需要的请求扭矩,输出到动力系统, 动力系统执行动力域控制器的指令驱动车辆

      - 控制器参数优化环路主要由智能体(也称为智能决策模块)及其输入输出接口构成.智能体根据车辆运动状态,油门踏板和刹车踏板状态,在满足常规控制环路控制指标的前提下,按照最优的降低能耗的目标动态对动力域控制器相关参数(比如刹车踏板请求扭矩映射表)进行调整.调整的方法可以是当前最先进或者典型的奖励驱动的强化学习方法比如基于策略梯度的方法(Policy Graident Method)或者基于Q学习的方法(Q-Learning Method)或者是基于模型的强化学习方法.

图 4

4.3 智能驾驶系统基于能效奖励驱动的智能驾驶纵向控制器参数优化系统。如图5所示。系统由两个环路组成, 一个是智能驾驶(包括辅助或者自动车辆)控制环路,另一个是控制器参数优化环路.

   - 智能驾驶控制环路由智能驾驶纵向控制器按运动规划算法计算出期望速度或加速信号或减速信号请求值发送给动力域控制器;动力域控制器控制器请求的速度或加速度或减速度信号,通过计算获得需要的请求扭矩,输出到动力系统, 动力系统执行动力域控制器的指令驱动车辆

   - 控制器参数优化环路主要由智能体(也称为智能决策模块)及其输入输出接口构成.智能体根据车辆运动状态,油门踏板和刹车踏板状态,在满足常规控制环路控制指标的前提下,按照最优的降低能耗的目标动态对智能驾驶纵向控制器相关参数(比如比例积分微分参数PID参数)进行调整.调整的方法可以是当前最先进或者典型的奖励驱动的强化学习方法比如基于策略梯度的方法(Policy Graident Method)或者基于Q学习的方法(Q-Learning Method)或者是基于模型的强化学习方法.

图 5


## 上述技术方案产生了什么技术效果

- 奖励驱动，无需标注，节省海量的标注资源和开销
- 适用复杂环境的优化目标，即带噪声的高维度非平稳随机过程
- 端到端，场景连续，无需对场景进行分类和识别
- 司机作为环境观测的一部分，无需对司机驾驶行为进行分类和识别
- 随机闭环控制：可利用输入的非平稳随机信号及时更新控制决策

- 在现有动力系统和辅助驾驶及系统的基础上增加智能决策模块
- 通过观测车辆的运动状态和司机对油门和刹车踏板的操作进行智能决策
- 通过观测车辆的运动状态和辅助驾驶系统的运动规划进行智能决策
- 以降低能耗作为奖励函数对动力系统或辅助驾驶系统的控制参数自动地确定最优静态值
- 以降低能耗作为奖励函数对动力系统或辅助驾驶系统的控制参数自动进行动态调整
- 通过与环境的交互在满足基本工况的前提下可获得能效优化策略的持续改进
- 最优能效目标的调整策略可适应道路状况的动态变化,及时作出反应



## 上述技术方案中可以替代的地方

- 优化对象，动力域控制器和智能驾驶纵向控制器，也可以替换成横向控制器，空调控制器，电池管理系统控制器等等。

- 优化指标，能效优化的目标可以替换成可数量化的舒适性或者安全性等等。


## 术语解释

## 参考文献

[1] Daniel Kahneman, Thinking, Fast and Slow. New York :Farrar, Straus and Giroux, 2011.
[2] David Silver, Satinder Singh Doina Precup, Richard S.Sutton, Reward is enough, 2021.
https://www.sciencedirect.com/science/article/pii/S0004370221000862
